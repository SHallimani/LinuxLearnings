I find its very important that we are clear about the part of Linux we are discussing this topic - Kernel Space or User Space

* ) Scope:
     The scope of mutex is within a process address space which has created it and is used for synchronization of common resource access.
     Whereas semaphore can be used across processes space and hence it can be used for interprocess synchronization/signaling.
     Hence mutex must be released by same thread which is taking it.
* ) Mutex is lightweight and faster than semaphore (only valid for userspace)
* ) Mutex can be acquired by same thread successfully multiple times with condition that it should release it same number of times.
    Other thread trying to acquire will block. Whereas in case of semaphore if same process tries to acquire it again it blocks as it can be acquired only once

-----------------
    Mutex:
-----------------
Mutual exclusion (mutex) is the de facto most used locking mechanism. To understand how it
works, let's see what its structure looks like in include/linux/mutex.h:

struct mutex {
 /* 1: unlocked, 0: locked, negative: locked, possible waiters */
 atomic_t count;
 spinlock_t wait_lock;
 struct list_head wait_list;
 [...]
};

As we have seen in the section wait queue, there is also a list type field in the structure:
wait_list. The principle of sleeping is the same.
Contenders are removed from the scheduler run queue and put onto the wait list (wait_list) in a
sleep state. The kernel then schedules and executes other tasks. When the lock is released, a
waiter in the wait queue is woken, moved off the wait_list, and scheduled back.

Here is an example of a mutex implementation:
struct mutex my_mutex;
mutex_init(&my_mutex);
/* inside a work or a thread */
mutex_lock(&my_mutex);
access_shared_memory();
mutex_unlock(&my_mutex);
Please have a look at include/linux/mutex.h in the kernel source to see the strict rules you must
respect with mutexes. Here are some of them:
        1/ Only one task can hold the mutex at a time; this is actually not a rule, but a fact
        2/ Multiple unlocks are not permitted
        3/ They must be initialized through the API
        4/ A task holding the mutex may not exit, since the mutex will remain locked, and possible contenders will wait (will sleep) forever
        5/ Memory areas where held locks reside must not be freed
        6/ Held mutexes must not be reinitialized
        7/ Since they involve rescheduling, mutexes may not be used in atomic contexts, such as tasklets and timers

Note:
As with wait_queue, there is no polling mechanism with mutexes. Every time thamutex_unlock is called on a mutex, the kernel checks for waiters in wait_list. If
any, one (and only one) of them is awakened and scheduled; they are woken in
the same order in which they were put to sleep

The mutex subsystem checks and enforces the following rules:
------------------------------------------------------------
    - Only one task can hold the mutex at a time.
    - Only the owner can unlock the mutex.
    - Multiple unlocks are not permitted.
    - Recursive locking/unlocking is not permitted.
    - A mutex must only be initialized via the API (see below).
    - A task may not exit with a mutex held.
    - Memory areas where held locks reside must not be freed.
    - Held mutexes must not be reinitialized.
    - Mutexes may not be used in hardware or software interrupt
      contexts such as tasklets and timers.

These semantics are fully enforced when CONFIG DEBUG_MUTEXES is enabled.
In addition, the mutex debugging code also implements a number of other
features that make lock debugging easier and faster:

    - Uses symbolic names of mutexes, whenever they are printed
      in debug output.
    - Point-of-acquire tracking, symbolic lookup of function names,
      list of all locks held in the system, printout of them.
    - Owner tracking.
    - Detects self-recursing locks and prints out all relevant info.
    - Detects multi-task circular deadlocks and prints out all affected
      locks and tasks (and only those tasks).

-----------------
    Semaphore:
-----------------

-----------------
    SpinLock:
-----------------
Like mutex, spinlock is a mutual exclusion mechanism; it only has two states:
        1/ locked (aquired)
        2/ unlocked (released)
Any thread that needs to acquire the spinlock will active loop until the lock is acquired, which
breaks out of the loop. This is the point where mutex and spinlock differ. Since spinlock heavily
consumes the CPU while looping, it should be used for very quick acquires, especially when
time to hold the spinlock is less than time to reschedule. Spinlock should be released as soon as
the critical task is done

In order to avoid wasting CPU time by scheduling a thread that may probably spin, trying to
acquire a lock held by another thread moved off the run queue, the kernel disables preemption
whenever a code holding a spinlock is running. With preemption disabled, we prevent the
spinlock holder from being moved off the run queue, which could lead waiting processes to spin
for a long time and consume CPU

As long as one holds a spinlock, other tasks may be spinning while waiting on it. By using
spinlock, you asserts and guarantee that it will not be held for a long time. You can say it is
better to spin in a loop, wasting CPU time, than the cost of sleeping your thread, context-shifting
to another thread or process, and being woken up afterward. Spinning on a processor means no
other task can run on that processor; it then makes no sense to use spinlock on a single core
machine. In the best case, you will slow down the system; in the worst case, you will deadlock,
as with mutexes. For this reason, the kernel just disables preemption in response to the
spin_lock(spinlock_t *lock) function on single processor. On a single processor (core) system,
you should use spin_lock_irqsave() and spin_unlock_irqrestore(), which will respectively disable
the interrupts on the CPU, preventing interrupt concurrency

Since you do not know in advance what system you will write the driver for, it is recommended
you acquire a spinlock using spin_lock_irqsave(spinlock_t *lock, unsigned long flags), which
disables interrupts on the current processor (the processor where it is called) before taking the
spinlock. spin_lock_irqsave internally calls local_irq_save(flags);, an architecture-dependent
function to save the IRQ status, and preempt_disable() to disable preemption on the relevant
CPU. You should then release the lock with spin_unlock_irqrestore(), which does the reverse
operations that we previously enumerated. This is a code that does lock acquire and release. It is
an IRQ handler, but let's just focus on the lock aspect. We will discuss more about IRQ handlers
in the next section:
/* some where */
spinlock_t my_spinlock;
spin_lock_init(my_spinlock);
static irqreturn_t my_irq_handler(int irq, void *data)
{
 unsigned long status, flags;
 spin_lock_irqsave(&my_spinlock, flags);
 status = access_shared_resources();
 spin_unlock_irqrestore(&gpio->slock, flags);
 return IRQ_HANDLED;
}

-----------------------------
   Spinlock versus mutexes
-----------------------------

Used for concurrency in the kernel, spinlocks and mutexes each have their own objectives:
        1/ Mutexes protect the process's critical resource, whereas spinlock protects the IRQ handler's critical sections
        2/ Mutexes put contenders to sleep until the lock is acquired, whereas spinlocks infinitely spin in a loop (consuming CPU) until the lock is acquired
        3/ Because of the previous point, you can't hold spinlock for a long time, since waiters will waste CPU time waiting for the lock, whereas a mutex can be held as long as the resource needs to be protected, since contenders are put to sleep in a wait queue
